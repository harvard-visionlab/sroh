{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sroh_2022_run_validation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNOrT8l8gOxkxjlfWlUI2C5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "06755360bfa548efa8a9a349dea88e3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_60de9be9b0064390b5ba75399164c209",
              "IPY_MODEL_ed9e86807d174588a281a5887e4ff54c",
              "IPY_MODEL_98fbec063fc64c1b96ef225876977be4"
            ],
            "layout": "IPY_MODEL_797fab179f18408c9d358a18b8f5ec1e"
          }
        },
        "60de9be9b0064390b5ba75399164c209": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a2e49a00d584a199b232a2320842e9a",
            "placeholder": "​",
            "style": "IPY_MODEL_35c68fd26141452295d11a9c6064e855",
            "value": "100%"
          }
        },
        "ed9e86807d174588a281a5887e4ff54c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d1c4cf336cb42c5b972c0d1a4afe324",
            "max": 499245218,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d28abc6a65e24c7aadaddac77388bab2",
            "value": 499245218
          }
        },
        "98fbec063fc64c1b96ef225876977be4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22c37f4271704ac9ab64f20b9b3d6739",
            "placeholder": "​",
            "style": "IPY_MODEL_a0bc768f001341f393e4db0be9d0d2ed",
            "value": " 476M/476M [00:37&lt;00:00, 14.8MB/s]"
          }
        },
        "797fab179f18408c9d358a18b8f5ec1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a2e49a00d584a199b232a2320842e9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35c68fd26141452295d11a9c6064e855": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d1c4cf336cb42c5b972c0d1a4afe324": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d28abc6a65e24c7aadaddac77388bab2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "22c37f4271704ac9ab64f20b9b3d6739": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0bc768f001341f393e4db0be9d0d2ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harvard-visionlab/sroh/blob/main/2022/sroh_2022_run_validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Imagnet-1k Validation\n",
        "\n",
        "For models trained on Imagenet classification, this script will run evaluate accuracy (top1, top5), and cross-entropy loss for the 50,000 imagenet validation images."
      ],
      "metadata": {
        "id": "X99nTSn1_Hmd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## download the validation set \n",
        "\n",
        "Run only once at the start of each session. Make sure you are on a GPU runtime! But also note that the download takes time, and Colab might complain that you aren't using the GPU and ask you to switch runtimes. You should just dismiss this suggestion."
      ],
      "metadata": {
        "id": "CW4Bc78I_ivA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -c https://www.dropbox.com/s/6vu07wtshpqpcr2/val.tar.gz\n",
        "!tar -xf val.tar.gz\n",
        "!rm val.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5ZuzkqwBNQj",
        "outputId": "11dccab4-04d1-42a6-ef5f-5945093d4784"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-07-21 13:52:23--  https://www.dropbox.com/s/6vu07wtshpqpcr2/val.tar.gz\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.81.18, 2620:100:6017:18::a27d:212\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.81.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/6vu07wtshpqpcr2/val.tar.gz [following]\n",
            "--2022-07-21 13:52:24--  https://www.dropbox.com/s/raw/6vu07wtshpqpcr2/val.tar.gz\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uce29521225558f9250c069972b3.dl.dropboxusercontent.com/cd/0/inline/BpgcJwJR9AW-nq4in-D0jD3L_pmLGZJpeCKDhHuEbBrep3QrMOA-Z1KG2QITk7V0IroCv363oGrH0a5nxzHC87BHczBw0pKdXyxkXd0GC9uCo3WaoeLIDiM5E_in0zrDQZGHXHvKRlSU4P7B0yffULm_9ClDQLVloxWPCaG3PxOPfw/file# [following]\n",
            "--2022-07-21 13:52:25--  https://uce29521225558f9250c069972b3.dl.dropboxusercontent.com/cd/0/inline/BpgcJwJR9AW-nq4in-D0jD3L_pmLGZJpeCKDhHuEbBrep3QrMOA-Z1KG2QITk7V0IroCv363oGrH0a5nxzHC87BHczBw0pKdXyxkXd0GC9uCo3WaoeLIDiM5E_in0zrDQZGHXHvKRlSU4P7B0yffULm_9ClDQLVloxWPCaG3PxOPfw/file\n",
            "Resolving uce29521225558f9250c069972b3.dl.dropboxusercontent.com (uce29521225558f9250c069972b3.dl.dropboxusercontent.com)... 162.125.80.15, 2620:100:6017:15::a27d:20f\n",
            "Connecting to uce29521225558f9250c069972b3.dl.dropboxusercontent.com (uce29521225558f9250c069972b3.dl.dropboxusercontent.com)|162.125.80.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/BpiHqFILPzyRCJUfgMHze9MT214m846SYPGjKscTlhen5boZMJqchbdFE4p7TZC6hrgOvG8qyZC7HP2KPf60CwMMmWHVk72d_jqcfm91DxsF1Iouf2vgwfPYFIKDnTATdY2AMmaXnuUYUXnBZBEdd_fv-BjmJqEdVAkhhb7WMMijJ4Mm0bWVJlLxEJdV3j_jwPLsNSx1VZSaZCGhxYZ5V_H__C_7T_-eWr88jjpktkYynqyh53WAXmv62sV1Wzq-HXY7hPo6rElcs-YwE0YKwvNodfT8CisDT44sBBBpfyEN_dvmVOXxb9ZYBgaV0XDzA1ZYk8w4UEqlkXL5jJA3RwGRA1HojNrGPxe_Sv2py1uLAgfU7fWjxh1w1CvJHj14J4d8kH-3MravwqQjDxuxFdHq-_8ORp7inhjahCi7ArS78w/file [following]\n",
            "--2022-07-21 13:52:26--  https://uce29521225558f9250c069972b3.dl.dropboxusercontent.com/cd/0/inline2/BpiHqFILPzyRCJUfgMHze9MT214m846SYPGjKscTlhen5boZMJqchbdFE4p7TZC6hrgOvG8qyZC7HP2KPf60CwMMmWHVk72d_jqcfm91DxsF1Iouf2vgwfPYFIKDnTATdY2AMmaXnuUYUXnBZBEdd_fv-BjmJqEdVAkhhb7WMMijJ4Mm0bWVJlLxEJdV3j_jwPLsNSx1VZSaZCGhxYZ5V_H__C_7T_-eWr88jjpktkYynqyh53WAXmv62sV1Wzq-HXY7hPo6rElcs-YwE0YKwvNodfT8CisDT44sBBBpfyEN_dvmVOXxb9ZYBgaV0XDzA1ZYk8w4UEqlkXL5jJA3RwGRA1HojNrGPxe_Sv2py1uLAgfU7fWjxh1w1CvJHj14J4d8kH-3MravwqQjDxuxFdHq-_8ORp7inhjahCi7ArS78w/file\n",
            "Reusing existing connection to uce29521225558f9250c069972b3.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6018775043 (5.6G) [application/octet-stream]\n",
            "Saving to: ‘val.tar.gz’\n",
            "\n",
            "val.tar.gz          100%[===================>]   5.60G  28.9MB/s    in 3m 34s  \n",
            "\n",
            "2022-07-21 13:56:00 (26.8 MB/s) - ‘val.tar.gz’ saved [6018775043/6018775043]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## validation code"
      ],
      "metadata": {
        "id": "xDrzG4gs_3qL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "zSrGAsMP9f-g"
      },
      "outputs": [],
      "source": [
        "import torch \n",
        "from enum import Enum\n",
        "from fastprogress.fastprogress import progress_bar \n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(model, val_loader):\n",
        "  num_images = len(val_loader.dataset)\n",
        "  assert num_images==50000, f\"Expected 50K images, got {num_images}\"  \n",
        "  criterion = torch.nn.CrossEntropyLoss()\n",
        "  model.eval()\n",
        "  losses = AverageMeter('Loss', ':.4e', Summary.NONE)\n",
        "  top1 = AverageMeter('Acc@1', ':6.2f', Summary.AVERAGE)\n",
        "  top5 = AverageMeter('Acc@5', ':6.2f', Summary.AVERAGE)\n",
        "\n",
        "  for i, (images, target) in enumerate(progress_bar(val_loader)):\n",
        "      images = images.cuda(device, non_blocking=True)\n",
        "      target = target.cuda(device, non_blocking=True)\n",
        "\n",
        "      # compute output\n",
        "      output = model(images)\n",
        "      loss = criterion(output, target)\n",
        "\n",
        "      # measure accuracy and record loss\n",
        "      acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "      losses.update(loss.item(), images.size(0))\n",
        "      top1.update(acc1[0], images.size(0))\n",
        "      top5.update(acc5[0], images.size(0))\n",
        "\n",
        "  return top1.avg.item(), top5.avg.item(), losses.avg\n",
        "\n",
        "class Summary(Enum):\n",
        "    NONE = 0\n",
        "    AVERAGE = 1\n",
        "    SUM = 2\n",
        "    COUNT = 3\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self, name, fmt=':f', summary_type=Summary.AVERAGE):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.summary_type = summary_type\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def all_reduce(self):\n",
        "        total = torch.FloatTensor([self.sum, self.count])\n",
        "        dist.all_reduce(total, dist.ReduceOp.SUM, async_op=False)\n",
        "        self.sum, self.count = total.tolist()\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
        "        return fmtstr.format(**self.__dict__)\n",
        "    \n",
        "    def summary(self):\n",
        "        fmtstr = ''\n",
        "        if self.summary_type is Summary.NONE:\n",
        "            fmtstr = ''\n",
        "        elif self.summary_type is Summary.AVERAGE:\n",
        "            fmtstr = '{name} {avg:.3f}'\n",
        "        elif self.summary_type is Summary.SUM:\n",
        "            fmtstr = '{name} {sum:.3f}'\n",
        "        elif self.summary_type is Summary.COUNT:\n",
        "            fmtstr = '{name} {count:.3f}'\n",
        "        else:\n",
        "            raise ValueError('invalid summary type %r' % self.summary_type)\n",
        "        \n",
        "        return fmtstr.format(**self.__dict__)\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res        "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## run validation on Alexnet\n",
        "\n",
        "The only important thing here is to make sure to use the \"correct\" validation transforms for your model. This usualy means just making sure that you use the right image size and normalization statistics. For torchvision models, the right transform is:\n",
        "\n",
        "```\n",
        "transform = transforms.Compose([\n",
        "  transforms.Resize(256),\n",
        "  transforms.CenterCrop(224),\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                       std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "WoNOqFpi_9EX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models, datasets, transforms \n",
        "\n",
        "transform = transforms.Compose([\n",
        "  transforms.Resize(256),\n",
        "  transforms.CenterCrop(224),\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                       std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "transform"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTdSIMgT-4xy",
        "outputId": "869339f7-8342-4aef-d67f-062f4dcc2bd2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Compose(\n",
              "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=None)\n",
              "    CenterCrop(size=(224, 224))\n",
              "    ToTensor()\n",
              "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_folder = './val'\n",
        "dataset = datasets.ImageFolder(image_folder, transform=transform)\n",
        "val_loader = torch.utils.data.DataLoader(dataset, \n",
        "                                         batch_size=256, \n",
        "                                         shuffle=False, \n",
        "                                         num_workers=2, \n",
        "                                         pin_memory=True)\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fSpbKipAlNf",
        "outputId": "1410bfe7-df61-481e-f142-050d359e9e33"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset ImageFolder\n",
              "    Number of datapoints: 50000\n",
              "    Root location: ./val\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               Resize(size=256, interpolation=bilinear, max_size=None, antialias=None)\n",
              "               CenterCrop(size=(224, 224))\n",
              "               ToTensor()\n",
              "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              "           )"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.alexnet(pretrained=True)\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTpbLTJ2A1UO",
        "outputId": "922bb327-0ddd-472a-9f15-08538d7b03b5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AlexNet(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
              "  (classifier): Sequential(\n",
              "    (0): Dropout(p=0.5, inplace=False)\n",
              "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top1, top5, loss = validate(model, val_loader)\n",
        "top1, top5, loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "wpsMT4-KBKV9",
        "outputId": "44846906-7417-43db-8897-351419eb34ad"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='196' class='' max='196' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [196/196 05:02<00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(56.5440, device='cuda:0'),\n",
              " tensor(79.1080, device='cuda:0'),\n",
              " 1.9101363607025146)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## supervised models from ipcl project"
      ],
      "metadata": {
        "id": "mN33F7yoD5Gd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# this model is supervised in the usual way,\n",
        "# except each image was presented 5 times more often (to make a fair\n",
        "# comparison with IPCL models)\n",
        "model, transform = torch.hub.load(\"harvard-visionlab/open_ipcl\", \n",
        "                                  \"alexnetgn_supervised_ref12_augset1_5x\")\n",
        "print(transform)\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbfbGnBeDfMT",
        "outputId": "42342b30-2777-4fc8-f02c-4088514e14d1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/harvard-visionlab_open_ipcl_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compose(\n",
            "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=None)\n",
            "    CenterCrop(size=(224, 224))\n",
            "    ToTensor()\n",
            "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
            ")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "alexnet_gn(\n",
              "  (conv_block_1): Sequential(\n",
              "    (0): Conv2d(3, 96, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2), bias=False)\n",
              "    (1): GroupNorm(32, 96, eps=1e-05, affine=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (conv_block_2): Sequential(\n",
              "    (0): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "    (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (conv_block_3): Sequential(\n",
              "    (0): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (1): GroupNorm(32, 384, eps=1e-05, affine=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "  )\n",
              "  (conv_block_4): Sequential(\n",
              "    (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (1): GroupNorm(32, 384, eps=1e-05, affine=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "  )\n",
              "  (conv_block_5): Sequential(\n",
              "    (0): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (ave_pool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
              "  (fc6): Sequential(\n",
              "    (0): Linear(in_features=9216, out_features=4096, bias=True)\n",
              "    (1): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "  )\n",
              "  (fc7): Sequential(\n",
              "    (0): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (1): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "  )\n",
              "  (fc8): Sequential(\n",
              "    (0): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_folder = './val'\n",
        "dataset = datasets.ImageFolder(image_folder, transform=transform)\n",
        "val_loader = torch.utils.data.DataLoader(dataset, \n",
        "                                         batch_size=256, \n",
        "                                         shuffle=False, \n",
        "                                         num_workers=2, \n",
        "                                         pin_memory=True)\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zd6Z_f99Ec3J",
        "outputId": "a48d72ea-24f9-42fc-e719-fd0f59e1e561"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset ImageFolder\n",
              "    Number of datapoints: 50000\n",
              "    Root location: ./val\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               Resize(size=256, interpolation=bilinear, max_size=None, antialias=None)\n",
              "               CenterCrop(size=(224, 224))\n",
              "               ToTensor()\n",
              "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              "           )"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top1, top5, loss = validate(model, val_loader)\n",
        "top1, top5, loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "9rlosoYpEhgt",
        "outputId": "3155d7bb-9fa4-4f9d-c830-a80dd050d82f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='196' class='' max='196' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [196/196 04:30<00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60.947998046875, 82.76799774169922, 1.677472384262085)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# and another version, trained with the usual 1x number of images\n",
        "model, transform = torch.hub.load(\"harvard-visionlab/open_ipcl\", \n",
        "                                  \"alexnetgn_supervised_ref13_augset1_1x\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 888,
          "referenced_widgets": [
            "06755360bfa548efa8a9a349dea88e3f",
            "60de9be9b0064390b5ba75399164c209",
            "ed9e86807d174588a281a5887e4ff54c",
            "98fbec063fc64c1b96ef225876977be4",
            "797fab179f18408c9d358a18b8f5ec1e",
            "2a2e49a00d584a199b232a2320842e9a",
            "35c68fd26141452295d11a9c6064e855",
            "4d1c4cf336cb42c5b972c0d1a4afe324",
            "d28abc6a65e24c7aadaddac77388bab2",
            "22c37f4271704ac9ab64f20b9b3d6739",
            "a0bc768f001341f393e4db0be9d0d2ed"
          ]
        },
        "id": "LTDzCxW0EkBH",
        "outputId": "d158b5fa-cb09-4fda-8760-0231af2b5e25"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/harvard-visionlab_open_ipcl_main\n",
            "Downloading: \"https://visionlab-pretrainedmodels.s3.amazonaws.com/project_instancenet/wusnet/alexnet_gn_supervised_final.pth.tar\" to /root/.cache/torch/hub/checkpoints/alexnetgn_supervised_ref13_augset1_1x-1ea33cba82.pth.tar\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/476M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "06755360bfa548efa8a9a349dea88e3f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "alexnet_gn(\n",
              "  (conv_block_1): Sequential(\n",
              "    (0): Conv2d(3, 96, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2), bias=False)\n",
              "    (1): GroupNorm(32, 96, eps=1e-05, affine=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (conv_block_2): Sequential(\n",
              "    (0): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "    (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (conv_block_3): Sequential(\n",
              "    (0): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (1): GroupNorm(32, 384, eps=1e-05, affine=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "  )\n",
              "  (conv_block_4): Sequential(\n",
              "    (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (1): GroupNorm(32, 384, eps=1e-05, affine=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "  )\n",
              "  (conv_block_5): Sequential(\n",
              "    (0): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (ave_pool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
              "  (fc6): Sequential(\n",
              "    (0): Linear(in_features=9216, out_features=4096, bias=True)\n",
              "    (1): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "  )\n",
              "  (fc7): Sequential(\n",
              "    (0): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (1): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "  )\n",
              "  (fc8): Sequential(\n",
              "    (0): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_folder = './val'\n",
        "dataset = datasets.ImageFolder(image_folder, transform=transform)\n",
        "val_loader = torch.utils.data.DataLoader(dataset, \n",
        "                                         batch_size=256, \n",
        "                                         shuffle=False, \n",
        "                                         num_workers=2, \n",
        "                                         pin_memory=True)\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QKbsRtiFnkE",
        "outputId": "c63f650d-01ee-4726-9b56-e12cf25cfc4a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset ImageFolder\n",
              "    Number of datapoints: 50000\n",
              "    Root location: ./val\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               Resize(size=256, interpolation=bilinear, max_size=None, antialias=None)\n",
              "               CenterCrop(size=(224, 224))\n",
              "               ToTensor()\n",
              "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              "           )"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top1, top5, loss = validate(model, val_loader)\n",
        "top1, top5, loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "6UH3FYFiFpAp",
        "outputId": "110701a5-1651-4a4f-f689-eff439f1342d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='196' class='' max='196' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [196/196 06:31<00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60.593997955322266, 82.19400024414062, 1.7449316347503663)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zZQFeMavGzv0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}