{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple_actxgrad_importance_of_features_wrt_loss.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPAwiq8TO1m6RFXB/MsmMu/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harvard-visionlab/sroh/blob/main/2022/simple_actxgrad_importance_of_features_wrt_loss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Fid6iLhPSx9C"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## data loader\n",
        "\n",
        "We want to pass imagenet data through the model, Im not sure how you access that data, but you need the official Imagenet dset on your file system, with 'train' & 'val' subfolders, with category-wise subfolders 'n01440764', 'n01443537' etc. point the following 'data_folder' variable to that imagenet path."
      ],
      "metadata": {
        "id": "dVXXc1yST-dg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#downloading validation dataset\n",
        "!wget -c https://www.dropbox.com/s/6vu07wtshpqpcr2/val.tar.gz\n",
        "!tar -xf val.tar.gz\n",
        "!rm val.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGvWRY8eXD8s",
        "outputId": "6e7a42ad-a262-4d74-d76f-ecbf57655792"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-08-12 14:11:08--  https://www.dropbox.com/s/6vu07wtshpqpcr2/val.tar.gz\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6016:18::a27d:112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/6vu07wtshpqpcr2/val.tar.gz [following]\n",
            "--2022-08-12 14:11:08--  https://www.dropbox.com/s/raw/6vu07wtshpqpcr2/val.tar.gz\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc66010023f1c6aed7e237c3387e.dl.dropboxusercontent.com/cd/0/inline/Bq0fld2m3c-SHo0FDlF6jveXumjxkW0Qy2Twz_woQD1PjtdyV_LZfKZFma2eV8VGJ8PhiRqMwiM6M47OwzyeyFrSqAtKd4CDdw7EVyBBqFkpI_CLqo2iI3B-M40n-XVaAJWfTRmKBpqvHDxn51Ubeu3JbcDqH0wAMyP2wpQpHM9Quw/file# [following]\n",
            "--2022-08-12 14:11:09--  https://uc66010023f1c6aed7e237c3387e.dl.dropboxusercontent.com/cd/0/inline/Bq0fld2m3c-SHo0FDlF6jveXumjxkW0Qy2Twz_woQD1PjtdyV_LZfKZFma2eV8VGJ8PhiRqMwiM6M47OwzyeyFrSqAtKd4CDdw7EVyBBqFkpI_CLqo2iI3B-M40n-XVaAJWfTRmKBpqvHDxn51Ubeu3JbcDqH0wAMyP2wpQpHM9Quw/file\n",
            "Resolving uc66010023f1c6aed7e237c3387e.dl.dropboxusercontent.com (uc66010023f1c6aed7e237c3387e.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\n",
            "Connecting to uc66010023f1c6aed7e237c3387e.dl.dropboxusercontent.com (uc66010023f1c6aed7e237c3387e.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/Bq0r0-PtngEoXpYAhI_wf1HQyWG1MlURTwTPioyfa_TEOt3LZCS1_AFMtxyybqZRrLoa63jq-zdbyUYZ-fv7G050QFx8sfSuaQlqH_NtIN3kYINsqrugVhory2FMwiqd-o5VH5NsVtMDhaPrQ8tmukcmewqhaNV073A34jsqKDRIa9-5cG6ykml6Ft0wgg8oW-Fr92_y4TAgV8AQ7NgkKL1qvdNfg62aYtjrtypSlqJlOO6GJGV2fDuL0eYbyhtE5N0QKfXNotziZN-Gp63-qoX9h2Qfsrt7VjtiTv4zhpYIPy1BI-PEJPG3UpkYnBoQOh78eGY4qp5o1FsjBAt1UoslDFyaSN4p54EcUyFQLO4rc-auv2LCH-ozZk7unPxt_sVRXKWEOZCASZoOVNnQuuUcRb4Q4pePSK0IID4sVyTM-g/file [following]\n",
            "--2022-08-12 14:11:10--  https://uc66010023f1c6aed7e237c3387e.dl.dropboxusercontent.com/cd/0/inline2/Bq0r0-PtngEoXpYAhI_wf1HQyWG1MlURTwTPioyfa_TEOt3LZCS1_AFMtxyybqZRrLoa63jq-zdbyUYZ-fv7G050QFx8sfSuaQlqH_NtIN3kYINsqrugVhory2FMwiqd-o5VH5NsVtMDhaPrQ8tmukcmewqhaNV073A34jsqKDRIa9-5cG6ykml6Ft0wgg8oW-Fr92_y4TAgV8AQ7NgkKL1qvdNfg62aYtjrtypSlqJlOO6GJGV2fDuL0eYbyhtE5N0QKfXNotziZN-Gp63-qoX9h2Qfsrt7VjtiTv4zhpYIPy1BI-PEJPG3UpkYnBoQOh78eGY4qp5o1FsjBAt1UoslDFyaSN4p54EcUyFQLO4rc-auv2LCH-ozZk7unPxt_sVRXKWEOZCASZoOVNnQuuUcRb4Q4pePSK0IID4sVyTM-g/file\n",
            "Reusing existing connection to uc66010023f1c6aed7e237c3387e.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6018775043 (5.6G) [application/octet-stream]\n",
            "Saving to: ‘val.tar.gz’\n",
            "\n",
            "val.tar.gz          100%[===================>]   5.60G  47.8MB/s    in 98s     \n",
            "\n",
            "2022-08-12 14:12:49 (58.6 MB/s) - ‘val.tar.gz’ saved [6018775043/6018775043]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_folder = '/content/val'"
      ],
      "metadata": {
        "id": "kZpWPD6ZT5N4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "kwargs = {'num_workers': 4, 'pin_memory': True, 'sampler':None} if 'cuda' in device else {}\n",
        "\n",
        "dset = ImageFolder(data_folder,transform=transform)\n",
        "print(dset)\n",
        "assert len(dset) == 50000, f\"Oops, expected 50000 images, got {len(dset)}\"\n",
        "\n",
        "dloader = DataLoader(dset,\n",
        "                     batch_size=256,\n",
        "                     shuffle=False,\n",
        "                     **kwargs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZAup4wtUW0h",
        "outputId": "0635748f-be99-4013-bee7-8f724494fea3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset ImageFolder\n",
            "    Number of datapoints: 50000\n",
            "    Root location: /content/val\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               Resize(size=256, interpolation=bilinear, max_size=None, antialias=None)\n",
            "               CenterCrop(size=(224, 224))\n",
            "               ToTensor()\n",
            "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
            "           )\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepping model\n",
        "\n",
        "We want to 'score' the features in a layer by how much they might affect the model loss. We can approximate that as the average activationxgrad passing through a feature. Intuitively, this works because the gradient measures how much 'changing' the feature would affect the loss, while the activation size measures how much the feature would change (setting a high activation feature to 0 activation is a big change). Well use a 'hook' to save activation/gradient values."
      ],
      "metadata": {
        "id": "ZIMHbAEjXu9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model, transform = torch.hub.load(\"harvard-visionlab/open_ipcl\", \"alexnetgn_supervised_ref13_augset1_1x\")\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yceu4-j0bsUW",
        "outputId": "2d5b5b7c-7807-4c48-9ce2-ac2261b6ccff"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/harvard-visionlab_open_ipcl_main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#little trick for getting a dictionary with reference names for each module in your model, at all nestings\n",
        "layers = dict([*model.named_modules()])\n",
        "#so now we can reference modules with a string;\n",
        "print(layers.keys())\n",
        "layers['fc6.0']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bq70ozVYUXDx",
        "outputId": "f1044826-e263-4529-ce2f-5194a4f5aa42"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['', 'conv_block_1', 'conv_block_1.0', 'conv_block_1.1', 'conv_block_1.2', 'conv_block_1.3', 'conv_block_2', 'conv_block_2.0', 'conv_block_2.1', 'conv_block_2.2', 'conv_block_2.3', 'conv_block_3', 'conv_block_3.0', 'conv_block_3.1', 'conv_block_3.2', 'conv_block_4', 'conv_block_4.0', 'conv_block_4.1', 'conv_block_4.2', 'conv_block_5', 'conv_block_5.0', 'conv_block_5.1', 'conv_block_5.2', 'conv_block_5.3', 'ave_pool', 'fc6', 'fc6.0', 'fc6.1', 'fc6.2', 'fc7', 'fc7.0', 'fc7.1', 'fc7.2', 'fc8', 'fc8.0'])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=9216, out_features=4096, bias=True)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn, Tensor\n",
        "from typing import Dict, Iterable, Callable\n",
        "\n",
        "class actgrad_extractor(nn.Module):\n",
        "    def __init__(self, model: nn.Module, layers: Iterable[str]):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.layers = layers\n",
        "        self.activations = {layer: None for layer in layers}\n",
        "        self.gradients = {layer: None for layer in layers}\n",
        "        self.hooks = {'forward':{},\n",
        "                      'backward':{}}   #saving hooks to variables lets us remove them later if we want\n",
        "        \n",
        "        for layer_id in layers:\n",
        "            layer = dict([*self.model.named_modules()])[layer_id]\n",
        "            self.hooks['forward'][layer_id] = layer.register_forward_hook(self.save_activations(layer_id)) #execute on forward pass\n",
        "            self.hooks['backward'][layer_id] = layer.register_backward_hook(self.save_gradients(layer_id))    #execute on backwards pass\n",
        "\n",
        "    def save_activations(self, layer_id: str) -> Callable:\n",
        "        def fn(module, input, output):  #register_hook expects to recieve a function with arguments like this\n",
        "            #output is what is return by the layer with dim (batch_dim x out_dim), sum across the batch dim\n",
        "            batch_summed_output = torch.sum(torch.abs(output),dim=0).detach().cpu()\n",
        "            if self.activations[layer_id] is None:\n",
        "                self.activations[layer_id] = batch_summed_output\n",
        "            else:\n",
        "                self.activations[layer_id] +=  batch_summed_output\n",
        "        return fn\n",
        "    \n",
        "    def save_gradients(self, layer_id: str) -> Callable:\n",
        "        def fn(module, grad_input, grad_output):\n",
        "            batch_summed_output = torch.sum(torch.abs(grad_output[0]),dim=0).detach().cpu() #grad_output is a tuple with 'device' as second item\n",
        "            if self.gradients[layer_id] is None:\n",
        "                self.gradients[layer_id] = batch_summed_output\n",
        "            else:\n",
        "                self.gradients[layer_id] +=  batch_summed_output \n",
        "        return fn\n",
        "    \n",
        "    def remove_all_hooks(self):      \n",
        "        for hook in self.hooks['forward'].values():\n",
        "            hook.remove()\n",
        "        for hook in self.hooks['backward'].values():\n",
        "            hook.remove()\n",
        "    "
      ],
      "metadata": {
        "id": "EBh6rJRKYDSb"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running model"
      ],
      "metadata": {
        "id": "8eEvs6ldYH5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_layers = [\"conv_block_3.0\",\"fc6.0\"]\n",
        "\n",
        "model_actgrad_extractor = actgrad_extractor(model, layers=target_layers)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "3CXxSIXGYFmM"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fastprogress import progress_bar \n",
        "\n",
        "iter_dataloader = iter(dloader)\n",
        "iters = len(iter_dataloader)  #if you want to test this out quickly just set this to a small number\n",
        "print('total batches: ' + str(iters)) \n",
        "for it in progress_bar(range(iters)):\n",
        "    inputs, target = next(iter_dataloader)\n",
        "    inputs = inputs.to(device)\n",
        "    target = target.to(device)\n",
        "\n",
        "    model.zero_grad()\n",
        "    \n",
        "    output = model(inputs)\n",
        "    \n",
        "    loss = criterion(output,target)\n",
        "    loss.backward()\n",
        "    \n",
        "model_actgrad_extractor.remove_all_hooks()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "YrsanaatYSWl",
        "outputId": "7e969343-878f-4067-9aff-fc095c6701db"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total batches: 196\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='196' class='' max='196' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [196/196 01:44&lt;00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get average by dividing result by length of dset\n",
        "activations = model_actgrad_extractor.activations\n",
        "gradients = model_actgrad_extractor.gradients\n",
        "\n",
        "for l in target_layers:\n",
        "    activations[l] /= len(dset)\n",
        "    gradients[l] /= len(dset)"
      ],
      "metadata": {
        "id": "fps7PwZ_YS__"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#scores are just actxgrad! Do with them what you want\n",
        "\n",
        "scores = {}\n",
        "for l in target_layers:\n",
        "    scores[l] = activations[l]*gradients[l]\n",
        "    print(l, scores[l].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3c8OySNYWzm",
        "outputId": "be2e4533-30cd-4637-da55-316362a5c6c1"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv_block_3.0 torch.Size([384, 13, 13])\n",
            "fc6.0 torch.Size([4096])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ftyfpu4bdjxW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}