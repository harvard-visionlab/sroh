{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "face_processing_deepnets.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "04WSoUi15bZL",
        "khtaDiiN5fZJ",
        "DFbX3cO-EJfi",
        "sEj3Fqej7ZTT",
        "cHGlV-O4Hcri",
        "VMQovMI04zPG",
        "8Gro7WjkHSh4",
        "LvPnPWHuK_uP",
        "BGypZ8lABR4Q"
      ],
      "private_outputs": true,
      "authorship_tag": "ABX9TyP08rHty4YDwB6evFbaZZG0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harvard-visionlab/sroh/blob/main/2022/face_processing_deepnets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloads and imports (only need to do these once at the start)"
      ],
      "metadata": {
        "id": "04WSoUi15bZL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwedH-NU42fp"
      },
      "outputs": [],
      "source": [
        "!wget -c -q https://raw.githubusercontent.com/harvard-visionlab/sroh/main/2022/feature_extractor.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install facenet-pytorch"
      ],
      "metadata": {
        "id": "KsRD4_K-5Jqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchsummaryX"
      ],
      "metadata": {
        "id": "RDQRjUwmKf7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kornia"
      ],
      "metadata": {
        "id": "XS5mgrveMs_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U git+https://github.com/albumentations-team/albumentations"
      ],
      "metadata": {
        "id": "KUYnXS8vOB99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p images/test_images\n",
        "!wget -c https://www.dropbox.com/s/03shxyunw62wxmk/Einstein.jpg -O ./images/test_images/Einstein.jpg\n",
        "!wget -c https://www.dropbox.com/s/gq55hwxsnzncnem/friends.zip\n",
        "!unzip friends.zip\n",
        "!mv friends ./images/friends"
      ],
      "metadata": {
        "id": "B4jegKl__z1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r ./__MACOSX\n",
        "!rm friends.zip"
      ],
      "metadata": {
        "id": "MEk-EQAf1LRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall opencv-python -y\n",
        "!pip install opencv-python"
      ],
      "metadata": {
        "id": "S9AyvddW3Irs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import cv2"
      ],
      "metadata": {
        "id": "toHzjPpF3Lxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step0 - Load Helper Functions\n",
        "\n",
        "These functions are used to setup and run the analysis."
      ],
      "metadata": {
        "id": "khtaDiiN5fZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## low-level helpers \n",
        "\n"
      ],
      "metadata": {
        "id": "DFbX3cO-EJfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import warnings\n",
        "import mimetypes\n",
        "from pathlib import Path \n",
        "from PIL import Image \n",
        "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
        "from glob import glob\n",
        "from natsort import natsorted\n",
        "from pdb import set_trace \n",
        "\n",
        "import torchvision.transforms.functional as F\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "image_extensions = set(\n",
        "    k for k, v in mimetypes.types_map.items() if v.startswith('image/'))\n",
        "\n",
        "def get_root_folder(image_source):\n",
        "    p = Path(image_source)\n",
        "    while p.stem.startswith('*'): p = p.parent\n",
        "    return p\n",
        "\n",
        "def get_files(filepath, extensions=None, recurse=False, sort=True):\n",
        "    '''Return list of files in `filepath` that have a suffix in `extensions`. `recurse` determines if we search subfolders. `sort` determines whether to sort files. '''\n",
        "    p = Path(filepath)\n",
        "    \n",
        "    pattern = '**/*' if recurse else '*'\n",
        "    if p.name.startswith('*') and p.name[1:].lower() in image_extensions:\n",
        "        pattern = '*'\n",
        "        extensions = p.name[1:] if extensions is None else extensions\n",
        "        p = p.parent\n",
        "        if p.stem.endswith('*'):\n",
        "            pattern = os.path.join(p.stem, pattern)\n",
        "            recurse = True\n",
        "            p = p.parent\n",
        "         \n",
        "    files = [o for o in p.glob(pattern)\n",
        "             if not o.name.startswith('.') and not o.is_dir()\n",
        "             and (extensions is None or (o.suffix.lower() in extensions))]\n",
        "    return sorted(files) if sort else files\n",
        "    \n",
        "def get_image_files(filepath, extensions=image_extensions, recurse=True, sort=True):\n",
        "    \"Return list of files in `filepath` that are images. defaults to include `image_extensions`.\"\n",
        "    return get_files(filepath, extensions=extensions, recurse=recurse, sort=sort)\n",
        "\n"
      ],
      "metadata": {
        "id": "Hdp-V0WeEGEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## image processing"
      ],
      "metadata": {
        "id": "sEj3Fqej7ZTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_cropped_faces(input_dir, image_size=224, margin=90):\n",
        "  # setup output directory\n",
        "  if os.path.isdir(input_dir):\n",
        "    output_dir = f\"{input_dir}_crop{image_size}_margin{margin}\"\n",
        "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "  else:\n",
        "    raise Exception(f\"Directory not found: {input_dir}\")\n",
        "    return\n",
        "\n",
        "  files = get_image_files(input_dir)\n",
        "  for file in files:\n",
        "    new_file = str(file).replace(str(Path(input_dir)), str(Path(output_dir)))\n",
        "    if os.path.isfile(new_file):\n",
        "      print(f\"==> file exists, skipping: {new_file}\")\n",
        "      continue\n",
        "    img = Image.open(file)\n",
        "    img_cropped = crop_face(img, image_size=image_size, margin=margin)\n",
        "    img_cropped.save(new_file)\n",
        "    print(f\"==> file saved: {new_file}\")\n",
        "\n",
        "  return output_dir\n",
        "  \n",
        "def prepare_identity_set(input_dir):\n",
        "  '''make copies of images with `mild` transforms that preserve identity,\n",
        "  like horizontal_flip, and subtle contrast or brightness variation'''\n",
        "\n",
        "  assert 'crop' in input_dir and 'margin' in input_dir, \\\n",
        "    f\"Expected input_dir should have cropped faces (foldername should contain `crop` and `margin`, got {input_dir}\"\n",
        "  # setup output directory\n",
        "  if os.path.isdir(input_dir):\n",
        "    output_dir = f\"{input_dir}_identity_set\"\n",
        "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "  else:\n",
        "    raise Exception(f\"Directory not found: {input_dir}\")\n",
        "    return\n",
        "\n",
        "  folders = []\n",
        "  files = [str(file) for file in get_image_files(input_dir)]\n",
        "  for file in files:\n",
        "    img = Image.open(file)\n",
        "\n",
        "    old_name = Path(str(Path(file).name))\n",
        "    stem, ext = old_name.stem, old_name.suffix \n",
        "    new_path = Path(file.replace(str(Path(input_dir)), str(Path(output_dir))))\n",
        "    new_path = os.path.join(new_path.parent, new_path.stem)\n",
        "    Path(new_path).mkdir(parents=True, exist_ok=True)\n",
        "    folders.append(new_path)\n",
        "\n",
        "    # save various copies\n",
        "    flip_levels =[0,1]\n",
        "    contrast_levels = [.8, .9, 1.0, 1.1, 1.2]\n",
        "    brightness_levels = [.8, .9, 1.0, 1.1, 1.2]\n",
        "    augs = []\n",
        "    for flip in flip_levels:\n",
        "      for c in contrast_levels:\n",
        "        for b in brightness_levels:\n",
        "          augs.append(f\"flip{flip}-c{c:1.1f}-b{b:1.1f}\")    \n",
        "\n",
        "    for idx,aug in enumerate(augs):\n",
        "      flip, contrast, brightness = aug.split(\"-\")\n",
        "      contrast_factor = float(contrast.replace(\"c\",\"\"))\n",
        "      brightness_factor = float(brightness.replace(\"b\",\"\"))\n",
        "      \n",
        "      new_file = os.path.join(new_path, f\"{stem}_{idx:02d}-{aug}{ext}\")\n",
        "\n",
        "      if os.path.isfile(new_file):\n",
        "        print(f\"==> file exists, skipping: {new_file}\")\n",
        "        continue\n",
        "\n",
        "      img = Image.open(file)\n",
        "      if flip==\"flip1\":\n",
        "        img = F.hflip(img)\n",
        "      img = F.adjust_contrast(img, contrast_factor = contrast_factor)\n",
        "      img = F.adjust_brightness(img, brightness_factor = brightness_factor)\n",
        "      \n",
        "      img.save(new_file)\n",
        "\n",
        "  return folders\n",
        "  \n",
        "def crop_face(img, image_size=224, margin=40):\n",
        "  '''A helper function to crop faces\n",
        "\n",
        "    First the shortest edge of the image is resized to 224\n",
        "    (preserving the aspect ratio). Then a face-detecting\n",
        "    network finds the face, and centers a crop around it,\n",
        "    resizing the entire image to 224 x 224.\n",
        "\n",
        "    Example usage:\n",
        "      img = Image.open('./images/test_images/Einstein.jpg')\n",
        "      img_cropped = crop_face(img, image_size=224, margin=80)\n",
        "  '''\n",
        "  mtcnn = MTCNN(image_size=image_size, margin=margin)\n",
        "  img = F.resize(img, image_size)\n",
        "  img_cropped = mtcnn(img)\n",
        "  img = Image.fromarray((img_cropped * 128 + 127.5).permute(1,2,0).numpy().astype(np.uint8))\n",
        "  return img\n"
      ],
      "metadata": {
        "id": "7fXhKemCpOpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9P1_wq7kpD1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## code for model loading"
      ],
      "metadata": {
        "id": "cHGlV-O4Hcri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch \n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import seaborn as sns\n",
        "\n",
        "from PIL import Image \n",
        "from torchvision import models, transforms\n",
        "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
        "from glob import glob\n",
        "from natsort import natsorted\n",
        "import torchvision.transforms.functional as F\n",
        "\n",
        "from torchsummaryX import summary\n",
        "\n",
        "def load_model(model_name):\n",
        "  if model_name == \"alexnet_imagenet\":\n",
        "    # alexnet trained on imagenet classification\n",
        "    model = models.alexnet(pretrained=True).eval()\n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
        "                                     std=[0.229, 0.224, 0.225])\n",
        "    model.layer_names = ['features.1','features.7', 'features.11','avgpool','classifier.2','classifier.5', 'classifier.6']\n",
        "\n",
        "  elif model_name == \"resnet50_imagenet\":\n",
        "    model = models.resnet50(pretrained=True).eval()\n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
        "                                     std=[0.229, 0.224, 0.225])\n",
        "    model.layer_names = ['relu', 'layer1', 'layer2', 'layer3', 'layer4', 'avgpool', 'fc']\n",
        "\n",
        "  elif model_name == \"inceptionV1_imagenet\":\n",
        "    model = models.googlenet(pretrained=True).eval()\n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
        "                                     std=[0.229, 0.224, 0.225])  \n",
        "    model.layer_names = ['conv2', \n",
        "                         'inception3a', 'inception3b', \n",
        "                         'inception4a', 'inception4d', \n",
        "                         'inception5a', 'inception5b', \n",
        "                         'avgpool', 'fc']\n",
        "\n",
        "  elif model_name == \"facenet_vggface2\":\n",
        "    # For a model pretrained on VGGFace2\n",
        "    model = InceptionResnetV1(pretrained='vggface2').eval()\n",
        "    normalize = transforms.Normalize(mean=[127.5/255, 127.5/255, 127.5/255], \n",
        "                                     std=[128/255, 128/255, 128/255])  \n",
        "    model.layer_names = ['conv2d_4b', 'repeat_2', 'repeat_2', 'repeat_3', 'block8', 'avgpool_1a', 'last_linear', 'last_bn']\n",
        "\n",
        "  elif model_name == \"facenet_casia\":\n",
        "    # For a model pretrained on CASIA-Webface\n",
        "    model = InceptionResnetV1(pretrained='casia-webface').eval()\n",
        "    normalize = transforms.Normalize(mean=[127.5/255, 127.5/255, 127.5/255], \n",
        "                                     std=[128/255, 128/255, 128/255])\n",
        "    model.layer_names = ['conv2d_4b', 'repeat_2', 'repeat_2', 'repeat_3', 'block8', 'avgpool_1a', 'last_linear', 'last_bn']\n",
        "\n",
        "  else:\n",
        "    raise Exception(f'model_name `{model_name}`` not supported')\n",
        "\n",
        "  return model, normalize\n",
        "\n"
      ],
      "metadata": {
        "id": "lyTeLauPHSFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## visualization helpers"
      ],
      "metadata": {
        "id": "VMQovMI04zPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%config InlineBackend.figure_format = 'retina'"
      ],
      "metadata": {
        "id": "1OfYF2QY5nkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math \n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "def show_images(images, max_cols=6, fig_width=15, row_height=5):\n",
        "  num_images = len(images)\n",
        "  num_rows = math.ceil(num_images / max_cols)\n",
        "  fig, axes = plt.subplots(num_rows, max_cols, figsize=(fig_width, num_rows*row_height))\n",
        "  if num_rows==1: axes = [axes]\n",
        "  c = 0\n",
        "  for row in axes:\n",
        "    for ax in row:\n",
        "      if c < num_images:\n",
        "        filename = images[c]\n",
        "        img = Image.open(filename)\n",
        "        ax.imshow(img)\n",
        "        ax.axis('off')\n",
        "        ax.set_title(Path(filename).stem)\n",
        "      else:\n",
        "        ax.remove() \n",
        "      c = c+1\n",
        "\n",
        "  return fig \n",
        "\n",
        "def show_image_folder(image_folder, max_cols=6, fig_width=15, row_height=5):\n",
        "  images = get_image_files(image_folder)\n",
        "  fig = show_images(images, max_cols=max_cols, fig_width=fig_width, row_height=row_height)  "
      ],
      "metadata": {
        "id": "ENX7nPGD40nC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step1 - Upload Images, Crop out Faces\n",
        "\n",
        "Create a subfolder within the \"images\" folder, e.g., \"experiment1-celebs\".\n",
        "\n",
        "Then run the `prepare_cropped_faces` script on those faces. You might have to adjust your \"margin\" to make sure you aren't zoomed in too much on the faces.\n",
        "\n",
        "The results are output to another folder, with the crop-size and margin appended to the folder name.\n",
        "\n",
        "e.g., if I ran:\n",
        "```\n",
        "prepare_cropped_faces('./images/test_images', image_size=224, margin=80)\n",
        "```\n",
        "\n",
        "The code will create cropped copies and place them in the folder ./images/test_images_crop224_margin80.\n",
        "\n",
        "Make sure to look at each face! If you are too zoomed in, or zoomed out, then you might need to adjust the margin. If the face-detector fails to find a face, then the code will crash. We should figure out how we want to handle these fail cases (e.g., manually crop them ourselves? elminate the image from the set?)."
      ],
      "metadata": {
        "id": "8Gro7WjkHSh4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create cropped copy of images in `test_images`\n",
        "output_dir = prepare_cropped_faces('./images/test_images', image_size=224, margin=90)\n",
        "output_dir"
      ],
      "metadata": {
        "id": "2nZ6Ha7uIyVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_image_folder(output_dir);"
      ],
      "metadata": {
        "id": "4KYSNxxH5J05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = prepare_cropped_faces('./images/friends', image_size=224, margin=90)\n",
        "output_dir"
      ],
      "metadata": {
        "id": "uo8MBsO61vOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_image_folder(output_dir, fig_width=15, row_height=4);"
      ],
      "metadata": {
        "id": "g-IayYKL2Tol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create cropped copy of images in `test_images`\n",
        "folders = prepare_identity_set('./images/test_images_crop224_margin90')"
      ],
      "metadata": {
        "id": "M3J6myl0idlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_image_folder(folders[0], fig_width=20, row_height=4)"
      ],
      "metadata": {
        "id": "IiRjvqVc923N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folders = prepare_identity_set('./images/friends_crop224_margin90')"
      ],
      "metadata": {
        "id": "HdidnNUI_-Dq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_image_folder(folders[0], fig_width=20, row_height=4)"
      ],
      "metadata": {
        "id": "Ku5warjiAErT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2 - Load a Model \n",
        "\n",
        "At this point, you've uploaded some face images, and prepared them by cropping them. Next, you want to load a model, and the appropriate normalization transform (which is model-specific! That's why we load the model and the normalize transform together).\n",
        "\n",
        "This code currently supports loading the following models, but we can add more. \n",
        "```\n",
        "    Available model_names (we could add more):\n",
        "      \"alexnet_imagenet\": alexnet architecture trained on imagenet classification      \n",
        "      \"resnet50_imagenet\": resnet50 architecture trained on imagenet classification\n",
        "      \"inceptionV1_imagenet\": resnet50 architecture trained on imagenet classification\n",
        "      \"facenet_vggface2\": inception_v1-like architecture trained on face recognition (using vggface2 dataset)\n",
        "      \"facenet_casia\": inception_v1-like architecture trained on face recognition (using casia-webface dataset)\n",
        "```\n",
        "\n",
        "I would start with `facenet_vggface2`, which is a very large deepnet (inception_v1-like architecture; google \"inception architecture\" and see if you can find a blog or youtube video explaining it).\n",
        "\n",
        "I'd start with this one because I think it's very good at face recognition, and so might have the most interesting face representations.\n",
        "\n",
        "At this point (Step2) we're just testing to make sure each model loads (and when we do this, PyTorch will download the weights and store a local copy so subsequent loads will be fast)."
      ],
      "metadata": {
        "id": "rU3sqoG-EUgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, normalize = load_model('facenet_vggface2')\n",
        "normalize"
      ],
      "metadata": {
        "id": "c-c-RmJe3aBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, normalize = load_model('facenet_casia')\n",
        "normalize"
      ],
      "metadata": {
        "id": "EP_T7aeAHXd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, normalize = load_model('inceptionV1_imagenet')\n",
        "normalize"
      ],
      "metadata": {
        "id": "YzuVeYCg7nWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model"
      ],
      "metadata": {
        "id": "_fSH8Nl8c1_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summary(model, torch.zeros((1, 3, 224, 224)))"
      ],
      "metadata": {
        "id": "TYxahYCpKakd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, normalize = load_model('alexnet_imagenet')\n",
        "normalize"
      ],
      "metadata": {
        "id": "GAQ2RX0dFtxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, normalize = load_model('resnet50_imagenet')\n",
        "normalize"
      ],
      "metadata": {
        "id": "lG4lcbl7cbKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3 - Setup Types of \"Distortion\"\n",
        "\n",
        "How will we distort the face? Inversion, blur, noise, rotation, sheer, vertical squish, horizontal squish, etc.\n",
        "\n",
        "To see a list of possibilities, we're using the albumentations library here, and they have a demo page: https://albumentations-demo.herokuapp.com/"
      ],
      "metadata": {
        "id": "LvPnPWHuK_uP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import albumentations.augmentations.transforms as AUG\n",
        "import albumentations.augmentations.geometric.transforms as GAUG\n",
        "import albumentations.augmentations.functional as AF\n",
        "import albumentations.augmentations.geometric.functional as AGF\n",
        "\n",
        "def distort_image(img, distortion_type, seed=6):\n",
        "  '''\n",
        "    available distortions:\n",
        "\n",
        "      invert: flip upside-down\n",
        "      rot90: rotate 90 degrees (counter clockwise)\n",
        "      blur: blur (15 by default)\n",
        "      blur20: blur by 20\n",
        "      blur<any_integer>: blur by <any_integer>\n",
        "  '''\n",
        "  if distortion_type==\"invert\":\n",
        "    img = img.rotate(180, Image.NEAREST, expand=0)\n",
        "  elif distortion_type==\"rot90\":\n",
        "    img = img.rotate(90, Image.NEAREST, expand=0)\n",
        "  elif distortion_type.startswith(\"blur\"):\n",
        "    if distortion_type==\"blur\":\n",
        "      blur_limit = 15\n",
        "    else:\n",
        "      blur_limit = int(distortion_type.replace(\"blur\",\"\"))\n",
        "    aug = AUG.Blur (blur_limit=blur_limit, always_apply=True, p=1.0)\n",
        "    img = Image.fromarray(aug(image=np.array(img))['image'])\n",
        "  elif distortion_type==\"downscale\":\n",
        "    aug = AUG.Downscale (scale_min=0.25, scale_max=0.25, interpolation=0, always_apply=True, p=1.0)\n",
        "    img = Image.fromarray(aug(image=np.array(img))['image'])\n",
        "  elif distortion_type==\"channel_shuffle\":\n",
        "    aug = AUG.ChannelShuffle(always_apply=True, p=1.0)\n",
        "    img = Image.fromarray(aug(image=np.array(img))['image'])\n",
        "  elif distortion_type==\"elastic\":\n",
        "    img = AGF.elastic_transform(np.array(img), alpha=1.0, \n",
        "                               sigma=50.0, alpha_affine=30.0, interpolation=0, \n",
        "                               border_mode=1, value=(0, 0, 0), random_state=np.random.RandomState(seed))\n",
        "    img = Image.fromarray(img)\n",
        "  elif distortion_type==\"color_invert\":\n",
        "    aug = AUG.InvertImg(always_apply=True, p=1.0)\n",
        "    img = Image.fromarray(aug(image=np.array(img))['image'])\n",
        "  elif distortion_type.startswith(\"grid_shuffle\"):\n",
        "    grid = tuple([int(v) for v in distortion_type.split(\"_\")[2].split(\"x\")])\n",
        "    aug = AUG.RandomGridShuffle(grid=grid, always_apply=True, p=1.0)\n",
        "    img = Image.fromarray(aug(image=np.array(img))['image'])       \n",
        "  return img"
      ],
      "metadata": {
        "id": "tBl3hvdiLRr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = Image.open('./images/test_images_crop224_margin90/Einstein.jpg').convert('RGB')\n",
        "img"
      ],
      "metadata": {
        "id": "CFDY6wE-Lt-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distort_image(img, \"invert\")"
      ],
      "metadata": {
        "id": "sa7XFPJ7MK_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distort_image(img, \"rot90\")"
      ],
      "metadata": {
        "id": "nsWXS74MLn58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distort_image(img, \"blur15\")"
      ],
      "metadata": {
        "id": "eNFL9IvcPij1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distort_image(img, \"downscale\")"
      ],
      "metadata": {
        "id": "EvDAdKLOObQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nothing happens because this is grayscale!\n",
        "distort_image(img, \"channel_shuffle\")"
      ],
      "metadata": {
        "id": "tnuZ4FvSQPW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# elastic does a \"random stretch\" each time; we can control the randomness (for reproducability), by setting a seed\n",
        "distort_image(img, \"elastic\", seed=6)"
      ],
      "metadata": {
        "id": "j47oWAFBSUKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distort_image(img, \"color_invert\")"
      ],
      "metadata": {
        "id": "AduRrrtCQ2x5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img"
      ],
      "metadata": {
        "id": "momhG2GAqNwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "F.adjust_contrast(img, contrast_factor=1.2)"
      ],
      "metadata": {
        "id": "_6BAQPdsXyP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "F.adjust_brightness(img, brightness_factor=0.8)"
      ],
      "metadata": {
        "id": "0Yl2iQiyqSEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distort_image(img, \"grid_shuffle_4x4\")"
      ],
      "metadata": {
        "id": "48oxmJljBRia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distort_image(img, \"grid_shuffle_10x10\")"
      ],
      "metadata": {
        "id": "MMIg3AAEMsT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distort_image(img, \"grid_shuffle_16x16\")"
      ],
      "metadata": {
        "id": "duTlOFIbMwpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step4 - Which Networks/Model Layers Capture Face Identity Best?\n",
        "\n",
        "Finally, an expeirment!\n",
        "\n",
        "Before we begin to \"strain\" our deepnets with challenging distortions, let's first examine how well these networks represent face identity in the first place.\n",
        "\n",
        "So which deepnets have the best face-identity representations? And which model layers have the best face-identity representations (early conv? middle conv? late conv? the first fully-connected layer? the penulatimate layer? the output layer?). \n",
        "\n",
        "A good face-identity representation should represent different copies of the same person as \"similar\" (if not identical), and pictures of different people as \"different\". In fact, the more different two people look, the more different their activations should be.\n",
        "\n",
        "We'll use representational similarity analysis to examine this in deepnets.\n",
        "\n",
        "We created 50 copies of each \"Friend\" (Phoebe, Monica, Rachel, Ross, Chandler, and Joey), yielding 300 images. We'll extract activations from a set of layers for each image. Then, for every pair of images (90000 pairs!), we'll compute the correlation in activations separately for each layer."
      ],
      "metadata": {
        "id": "BGypZ8lABR4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import seaborn as sns \n",
        "from collections import defaultdict \n",
        "from pdb import set_trace \n",
        "from feature_extractor import get_layer_names, FeatureExtractor\n",
        "\n",
        "def run_face_identity_experiment(model_name, image_folder, layer_names=None):\n",
        "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "  print(f\"==> loading images: {image_folder}\")\n",
        "  images = get_image_files(image_folder)\n",
        "  ids = np.array([img.parent.name for img in images])\n",
        "\n",
        "  print(f\"==> loading model: {model_name}\")\n",
        "  model, normalize = load_model(model_name)\n",
        "  model.to(device)\n",
        "  layer_names = model.layer_names if layer_names is None else layer_names\n",
        "\n",
        "  print(\"==> preparing image batch\")\n",
        "  transform = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    normalize,\n",
        "  ])\n",
        "  print(transform)\n",
        "  batch = torch.stack([transform(Image.open(img).convert('RGB')) for img in images])\n",
        "  batch = batch.to(device)\n",
        "\n",
        "  print(\"==> extracting activations\")\n",
        "  RDMS = compute_rdms(model, layer_names, batch, include_output=True)\n",
        "\n",
        "  print(\"==> summarizing rdms\")\n",
        "  df = summarize_rdms(model_name, RDMS, ids)\n",
        "\n",
        "  print(\"==> Done!\")\n",
        "\n",
        "  return RDMS, df, images, ids\n",
        "\n",
        "def compute_rdms(model, layer_names, batch, include_output=True):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    with FeatureExtractor(model, layers=layer_names) as extractor:\n",
        "      features = extractor(batch)\n",
        "\n",
        "    if include_output:\n",
        "      output = model(batch)\n",
        "      features['output'] = output.cpu().clone().detach()\n",
        "\n",
        "    RDMS = {}\n",
        "    for layer_num,(layer_name,X) in enumerate(features.items()):\n",
        "      X = X.flatten(start_dim=1)\n",
        "      rdm = torch.corrcoef(X)\n",
        "      RDMS[layer_name] = rdm\n",
        "\n",
        "  return RDMS\n",
        "\n",
        "def summarize_rdms(model_name, RDMS, ids):\n",
        "  id_list = np.unique(ids)\n",
        "  results = defaultdict(list)\n",
        "\n",
        "  for layer_name,rdm in RDMS.items():\n",
        "    within_corrs = {}\n",
        "    for identity1 in id_list:\n",
        "      idxs1 = identity1 == ids\n",
        "      n = idxs1.sum()  \n",
        "\n",
        "      # store coorelations between all copies of identity1\n",
        "      subset = rdm[idxs1,:][:,idxs1]\n",
        "      vals = subset[np.triu_indices(n, k=1)]\n",
        "      within_corrs[identity1] = vals.clone()\n",
        "\n",
        "      for identity2 in id_list:\n",
        "        same_different = 'same-identity' if identity1==identity2 else 'different-identity'\n",
        "        idxs2 = identity2 == ids\n",
        "        subset = rdm[idxs1,:][:,idxs2]\n",
        "        vals = subset[np.triu_indices(n, k=1)]\n",
        "\n",
        "        results['model_name'].append(model_name)\n",
        "        results['layer_name'].append(layer_name)\n",
        "        results['identity1'].append(identity1)\n",
        "        results['identity2'].append(identity2)\n",
        "        results['same_different'].append(same_different)\n",
        "        results['mean_pearsonr'].append(vals.mean().item())\n",
        "        \n",
        "        if identity1==identity2:\n",
        "          results['dprime'].append(np.nan)\n",
        "        else:\n",
        "          dprime = compute_dprime(within_corrs[identity1], vals)\n",
        "          results['dprime'].append(dprime)\n",
        "\n",
        "  columns=['model_name', 'layer_name','identity1','identity2','same_different','mean_pearsonr','dprime']\n",
        "  df = pd.DataFrame(results, columns=columns)\n",
        "\n",
        "  return df\n",
        "\n",
        "def compute_dprime(a_vals, b_vals):\n",
        "  a_mean = a_vals.mean() \n",
        "  b_mean = b_vals.mean()\n",
        "  a_var = a_vals.numpy().var(ddof=1)\n",
        "  b_var = b_vals.numpy().var(ddof=1)\n",
        "  sd_pooled = np.sqrt((a_var+b_var)/2)\n",
        "  dprime = (a_mean-b_mean)/sd_pooled\n",
        "\n",
        "  return dprime.item()\n",
        "\n",
        "def compute_distortion_index(same_identity, diff_identity, same_disorted):\n",
        "  dprime_distorted = compute_dprime(same_identity, same_disorted)\n",
        "  dprime_diff = compute_dprime(same_identity, diff_identity)\n",
        "\n",
        "  return dprime_distorted / dprime_diff\n",
        "\n",
        "def barplot_same_vs_different(df):\n",
        "  model_name = df.iloc[0].model_name\n",
        "  sns.set(rc={'figure.figsize':(14,6)});\n",
        "  sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
        "  ax = sns.barplot(data=df, x=\"layer_name\", y=\"mean_pearsonr\", hue=\"same_different\")\n",
        "  ax.set_xlabel('same vs. different identity')\n",
        "  ax.set_title(f'{model_name} results')\n",
        "  plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.);\n",
        "  return ax\n",
        "\n",
        "def barplot_dprime(df):\n",
        "  model_name = df.iloc[0].model_name\n",
        "  sns.set(rc={'figure.figsize':(14,6)});\n",
        "  sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
        "  ax = sns.barplot(data=df, x=\"layer_name\", y=\"dprime\");\n",
        "  ax.set_title(f'{model_name} results: dprime vs. model layer (higher dprime => more identity-representation)');\n",
        "  # plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.);\n",
        "  ax.set_ylim([0,50.0])\n",
        "  return ax\n",
        "\n",
        "def plot_rdms(RDMS,ids):\n",
        "  print(list(np.unique(ids)))\n",
        "  for layer_name,rdm in RDMS.items():\n",
        "    ax = sns.heatmap(RDMS[layer_name], square=True, vmin=-.2, vmax=1);\n",
        "    ax.set_title(f\"{layer_name}\");    \n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ig1ryyifSQEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "    Available model_names (we could add more):\n",
        "      \"alexnet_imagenet\": alexnet architecture trained on imagenet classification      \n",
        "      \"resnet50_imagenet\": resnet50 architecture trained on imagenet classification\n",
        "      \"inceptionV1_imagenet\": resnet50 architecture trained on imagenet classification\n",
        "      \"facenet_vggface2\": inception_v1-like architecture trained on face recognition (using vggface2 dataset)\n",
        "      \"facenet_casia\": inception_v1-like architecture trained on face recognition (using casia-webface dataset)\n",
        "```"
      ],
      "metadata": {
        "id": "4er1sExmIKBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'facenet_vggface2'\n",
        "image_folder = \"./images/friends_crop224_margin90_identity_set\"\n",
        "RDMS, df, images, ids = run_face_identity_experiment(model_name, image_folder)"
      ],
      "metadata": {
        "id": "eLZWOLgsWyup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check which model layers were analyzed\n",
        "df.layer_name.unique()"
      ],
      "metadata": {
        "id": "Y3e-39fVYfPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the avg same-identity correlation vs. average diff-identity correlation\n",
        "ax = barplot_same_vs_different(df)"
      ],
      "metadata": {
        "id": "0AYP_3FCXo-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### dprime measure\n",
        "\n",
        "Another way of thinking about the bar graphs above is that they each actually summarize a distribution. e.g., For the \"output\" layer, there the blue bar represents all the pair-wise correlations between imges that depict the same person. We should also \"histogram\" those correlations, and do the same thing for the diff-identity distribution. That might look something like this:"
      ],
      "metadata": {
        "id": "JjCD9cSHIoDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "same_identity = .7 + torch.randn(2000)*.1\n",
        "diff_identity = .2 + torch.randn(2000)*.1\n",
        "\n",
        "dprime = compute_dprime(same_identity, diff_identity)\n",
        "ax = sns.distplot(diff_identity);\n",
        "sns.distplot(same_identity, ax=ax);\n",
        "\n",
        "ax.set_title(f\"Simulated results (dprime={dprime:3.3f})\");\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., labels=['diff-identity', 'same-identity']);"
      ],
      "metadata": {
        "id": "LN0Ibkm7ImZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can quantify the difference between distributions like this by taking a \"dprime\" measure, which is basically the distance between the distributions means (mean1 - mean2), divided by their average standard deviation (technically pooled variance: sqrt( (var1+var2)/2 ))."
      ],
      "metadata": {
        "id": "5FiSLD9HJb07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot our d-prime results\n",
        "ax = barplot_dprime(df)"
      ],
      "metadata": {
        "id": "I4zsaoQWZsXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# here we plot the pairwise correlation between each image in each layer\n",
        "# All 50 faces per person are next to each other (e.g., the first 50 are chandler)\n",
        "# So the correlations should be along 50x50 chunks on the diagonal.\n",
        "# But a good face representation would also have low correlations off the \n",
        "# diagonal (diff-identity pairs).\n",
        "plot_rdms(RDMS, ids)"
      ],
      "metadata": {
        "id": "9O66t72xHO0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GJFoUWRnLLY2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step5 - Which Distortions Have the Biggest Effect on Face-Identity Representation?\n",
        "\n",
        "OK, now we know that `facenet_vggface2` has the strongest face-identity features. Will it also be the most robust to face distortions? Or do FaceNet models have \"more face-specialized tuning\" making them more sensitive to distortions?\n",
        "\n",
        "To answer this question, we need to quantify the impact of face distortion. Suppose we take all 50 faces per identity (300 images), AND a distorted copy of each (another 300). We compute all the pairwise correlations between images with the same identity (same-identity), with different identities (diff-identity), and between an image and those with the same identity but distored (same distorted). That's a lot of correlations, but we could summarize them with three histograms (one for each condition). Below I show some \"simulated\" possible outcomes for a give distortion."
      ],
      "metadata": {
        "id": "TgP3n6QtgqAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "same_identity = .7 + torch.randn(2000)*.1\n",
        "diff_identity = .2 + torch.randn(2000)*.1\n",
        "same_disorted = .3 + torch.randn(2000)*.1\n",
        "\n",
        "distortion_index = compute_distortion_index(same_identity, diff_identity, same_disorted)\n",
        "\n",
        "ax = sns.distplot(diff_identity);\n",
        "sns.distplot(same_disorted, ax=ax);\n",
        "sns.distplot(same_identity, ax=ax);\n",
        "\n",
        "ax.set_title(f\"Simulated 'large effect' of Distortion (index={distortion_index:3.3f})\");\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., labels=['diff-identity', 'same-distorted', 'same-identity']);"
      ],
      "metadata": {
        "id": "eHbTeJblgn64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "same_identity = .7 + torch.randn(2000)*.1\n",
        "diff_identity = .2 + torch.randn(2000)*.1\n",
        "same_disorted = .6 + torch.randn(2000)*.1\n",
        "distortion_index = compute_distortion_index(same_identity, diff_identity, same_disorted)\n",
        "\n",
        "ax = sns.distplot(diff_identity);\n",
        "sns.distplot(same_disorted, ax=ax);\n",
        "sns.distplot(same_identity, ax=ax);\n",
        "\n",
        "ax.set_title(f\"Simulated 'small effect' of Distortion (index={distortion_index:3.3f})\");\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., labels=['diff-identity', 'same-distorted', 'same-identity']);"
      ],
      "metadata": {
        "id": "HJq9sFWhkTwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the effects can fall on a spectrum, from no effect (same-identity and same-distorted histograms on top of each other), to a massive effect (diff-identity and same-distorted on top of each other), or anywhere in between. It would be handy if we could compute the \"distortion impact\" on a scale from 0 (no impact; same-identity and same-distorted on top of each other) to 1 (full impact; diff-identity and same-disorted on top of each other).\n",
        "\n",
        "Recall that we can compute the separation between two distributions using a \"dprime\" measure\n",
        "```\n",
        "  dprime = (MeanDist1 - MeanDist2) / StdPooled\n",
        "```\n",
        "Basically, the difference in the mean of the two distributions, divided by the variance (since you have two distributions, you pool their variances).\n",
        "\n",
        "For our index, we compute dprime between the same-identity and same-distorted distributions, and compare it to the dprime for the same-identity vs. diff-identity distributions. This ratio will be close to zero when the same-identity vs. same-distorted distributions lie on top of each other, and will be one when the same-distorted distribution lies on top of the diff-identity distribution. It can even be higher than one (which would mean the distorted faces are more different from normal faces than a random other face...perhaps out of the realm of faces!)."
      ],
      "metadata": {
        "id": "IGYeiKAblGlo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing Distortion-Index for Different Distortions\n",
        "\n",
        "OK, great, now we just have to compute the distortion index for each of the distortions that we're interested in.\n",
        "\n"
      ],
      "metadata": {
        "id": "kZCDUTJizimm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_distortion_experiment(model_name, image_folder, distortion_name, \n",
        "                              layer_names=None):\n",
        "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "  print(f\"==> loading images: {image_folder}\")\n",
        "  images = get_image_files(image_folder)\n",
        "  ids = np.array([img.parent.name for img in images])\n",
        "\n",
        "  print(f\"==> loading model: {model_name}\")\n",
        "  model, normalize = load_model(model_name)\n",
        "  model.to(device)\n",
        "  layer_names = model.layer_names if layer_names is None else layer_names\n",
        "\n",
        "  print(\"==> preparing image batch (originals)\")\n",
        "  imgs = [Image.open(img).convert('RGB') for img in images]\n",
        "\n",
        "  transform1 = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    normalize,\n",
        "  ])\n",
        "  print(transform1)\n",
        "  batch1 = torch.stack([transform1(img) for img in imgs])\n",
        "\n",
        "  print(\"==> preparing distorted images\")\n",
        "  transform2 = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.CenterCrop(224),\n",
        "    lambda img: distort_image(img, distortion_name),\n",
        "    transforms.ToTensor(),\n",
        "    normalize,\n",
        "  ])\n",
        "  print(transform2)\n",
        "  batch2 = torch.stack([transform2(img) for img in imgs])\n",
        "\n",
        "  # concatenate\n",
        "  batch = torch.cat([batch1, batch2])\n",
        "  batch = batch.to(device)\n",
        "  print(batch.shape)\n",
        "\n",
        "  print(\"==> extracting activations\")\n",
        "  RDMS = compute_rdms(model, layer_names, batch, include_output=True)\n",
        "\n",
        "  print(\"==> summarizing rdms\")\n",
        "  df = summarize_distortion(model_name, RDMS, ids)\n",
        "\n",
        "  return RDMS, df, images, ids\n",
        "\n",
        "def summarize_distortion(model_name, RDMS, ids):\n",
        "  N = len(ids)\n",
        "  uuids = np.unique(ids)\n",
        "  corrs = defaultdict(dict)\n",
        "  results = defaultdict(list)\n",
        "  for layer_name, rdm in RDMS.items():\n",
        "    for identity1 in uuids:\n",
        "      idxs1 = identity1 == ids\n",
        "      n = idxs1.sum()  \n",
        "\n",
        "      # get same-identity values\n",
        "      subset = rdm[0:N,0:N] # just the part with the undistorted faces\n",
        "      subset = subset[idxs1,:][:,idxs1] # rows and columns for this identity\n",
        "      same_identity = subset[np.triu_indices(n, k=1)] # upper triangle of the subset\n",
        "      corrs[layer_name]['same_identity'] = same_identity.clone()\n",
        "\n",
        "      # get diff-identity values\n",
        "      subset = rdm[0:N,0:N] # just the part with the undistorted faces\n",
        "      subset = subset[idxs1,:][:,~idxs1] # rows for this identity, columns for all the others\n",
        "      mask = np.triu(torch.ones_like(subset),k=1)==1 # get upper tri of rectangular array\n",
        "      diff_identity = torch.tensor(subset.numpy()[mask])\n",
        "      corrs[layer_name]['diff_identity'] = diff_identity.clone()\n",
        "\n",
        "      # get same-distorted values\n",
        "      subset = rdm[N:,N:] # just the part with the distorted faces\n",
        "      subset = subset[idxs1,:][:,idxs1] # rows and columns for this identity\n",
        "      same_distorted = subset[np.triu_indices(n, k=1)] # upper triangle of the subset\n",
        "      corrs[layer_name]['same_distorted'] = same_distorted.clone()\n",
        "\n",
        "      distortion_index = compute_distortion_index(same_identity, diff_identity, same_distorted)\n",
        "\n",
        "      results['model_name'].append(model_name)\n",
        "      results['layer_name'].append(layer_name)\n",
        "      results['identity'].append(identity1)\n",
        "      results['same_count'].append(len(same_identity))\n",
        "      results['diff_count'].append(len(diff_identity))\n",
        "      results['distort_count'].append(len(same_distorted))\n",
        "      results['distortion_index'].append(distortion_index)\n",
        "      results['same_identity'].append(same_identity.mean().item())\n",
        "      results['diff_identity'].append(diff_identity.mean().item())\n",
        "      results['same_distorted'].append(same_distorted.mean().item())\n",
        "\n",
        "  columns = ['model_name', 'layer_name', 'identity', 'same_count',\n",
        "            'diff_count', 'distort_count', 'distortion_index', \n",
        "             'same_identity', 'diff_identity', 'same_distorted']\n",
        "             \n",
        "  df = pd.DataFrame(results, columns=columns)\n",
        "\n",
        "  return df\n"
      ],
      "metadata": {
        "id": "o_n6NvX50C7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "  \n",
        "```"
      ],
      "metadata": {
        "id": "CbbJK5sIWA15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'facenet_vggface2'\n",
        "# model_name = 'alexnet_imagenet'\n",
        "# model_name = 'facenet_casia'\n",
        "# model_name = \"resnet50_imagenet\"\n",
        "# model_name = \"inceptionV1_imagenet\"\n",
        "image_folder = \"./images/friends_crop224_margin90_identity_set\"\n",
        "\n",
        "# see step 3 for possible distortions (we can add more)\n",
        "# distortion_name = 'invert'\n",
        "# distortion_name = 'elastic' \n",
        "distortion_name = \"grid_shuffle_6x6\"\n",
        "\n",
        "RDMS, df, images, ids = run_distortion_experiment(model_name, image_folder, distortion_name)\n"
      ],
      "metadata": {
        "id": "UU0_xlZ8lGCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "hxD5WFeRNb79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.lineplot(data=df, x=\"layer_name\", y=\"same_identity\")\n",
        "sns.lineplot(data=df, x=\"layer_name\", y=\"diff_identity\")\n",
        "sns.lineplot(data=df, x=\"layer_name\", y=\"same_distorted\")"
      ],
      "metadata": {
        "id": "dfrUPoGAN7qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ax = sns.barplot(data=df, x=\"layer_name\", y=\"distortion_index\")\n",
        "ax.set_ylim([-.1,.5]);"
      ],
      "metadata": {
        "id": "Z-8bEjbV4Yf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "bB5zwpFVMbx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ax = sns.distplot(diff_identity);\n",
        "# sns.distplot(same_distorted, ax=ax);\n",
        "# sns.distplot(same_identity, ax=ax);\n",
        "\n",
        "# ax.set_title(f\"{model_name}:{layer_name} (index={distortion_index:3.3f})\");\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., labels=['diff-identity', 'same-distorted', 'same-identity']);"
      ],
      "metadata": {
        "id": "NsgRopaEP5bQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "Based on these results I would say that the `facenet_vggface2` model has BY FAR the best \"face identity representations\". In later model layers, the correlation between \"same-identity\" faces was substantially higher than between \"different-identity\" faces (dprime reaching 40-something which is VERY HIGH).\n",
        "\n",
        "In contrast, imagenet-trained models like `alexnet_imagenet`, `resnet50_imagenet`, and even `inceptionV1_imagenet` (which is a very similar architecture to the facenet models), all showed much less of a separation between same-identity and different-identity correlations (...)."
      ],
      "metadata": {
        "id": "wv27-6icfK24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KATRpZS1fKHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ignore here down...will modify shuffle grid to be reproducible"
      ],
      "metadata": {
        "id": "8sjHrvwUXztr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img = Image.open('./images/test_images_crop224_margin90/Einstein.jpg').convert('RGB')\n",
        "img"
      ],
      "metadata": {
        "id": "A9SxLET4L4qK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AUG.RandomGridShuffle??"
      ],
      "metadata": {
        "id": "XQQAxh64UcVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aug = AUG.RandomGridShuffle(grid=(10,10), always_apply=True, p=1.0)\n",
        "aug"
      ],
      "metadata": {
        "id": "bSyUoG0eVV6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tiles = aug.get_params_dependent_on_targets(dict(image=np.array(img)))\n"
      ],
      "metadata": {
        "id": "UIY7PHjcVcxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img2 = AF.swap_tiles_on_image(np.array(img), tiles['tiles'])\n",
        "img2.shape"
      ],
      "metadata": {
        "id": "g8O6F1yyWVln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Image.fromarray(img2)"
      ],
      "metadata": {
        "id": "lDRCy3WsWvj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mev_Jy7vW5Vr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}