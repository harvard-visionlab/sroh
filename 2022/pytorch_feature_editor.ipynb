{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_feature_editor.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMjCQMAJz82jAf3dCM4oA6X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harvard-visionlab/sroh/blob/main/2022/pytorch_feature_editor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IZ3iFclE5tI-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn \n",
        "\n",
        "'''\n",
        "    FeatureEditor\n",
        "    \n",
        "    A wrapper class that will handle 'hooking' the to-be-lesioned layer(s),\n",
        "    editing them on the forward pass through the model, and returning\n",
        "    the lesioned outputs.\n",
        "    \n",
        "    For each layer you are editing, include a mask. That mask will\n",
        "    be multiplied by the layer's output, and the result will be\n",
        "    passed onto the next layer.\n",
        "    \n",
        "    Like FeatureExtractor, FeatureEditor should be used as a context\n",
        "    manager to clean up (remove) hooks when you are done with them:\n",
        "    \n",
        "    with FeatureEditor(model,layers,masks,return_features=False) as editor:\n",
        "        output = editor(imgs)\n",
        "        \n",
        "'''\n",
        "class FeatureEditor(nn.Module):\n",
        "    def __init__(self, model, layers, masks, return_features=False):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.layers = [layers] if isinstance(layers, str) else layers\n",
        "        self.masks = [masks] if not isinstance(layers, list) else masks\n",
        "        self.return_features = return_features\n",
        "        self._features = {layer: torch.empty(0) for layer in layers}\n",
        "        self.hooks = {}\n",
        "\n",
        "    def hook_layers(self):        \n",
        "        self.remove_hooks()\n",
        "        for layer_id,mask in zip(self.layers, self.masks):\n",
        "            layer = dict([*self.model.named_modules()])[layer_id]\n",
        "            self.hooks[layer_id] = layer.register_forward_hook(self.edit_outputs_hook(layer_id,mask))\n",
        "    \n",
        "    def remove_hooks(self):\n",
        "        for layer_id in self.layers:\n",
        "            self._features[layer_id] = torch.empty(0)\n",
        "            if layer_id in self.hooks:\n",
        "                self.hooks[layer_id].remove()\n",
        "                del self.hooks[layer_id]\n",
        "    \n",
        "    def __enter__(self, *args): \n",
        "        self.hook_layers()\n",
        "        return self\n",
        "    \n",
        "    def __exit__(self, *args): \n",
        "        self.remove_hooks()\n",
        "            \n",
        "    def edit_outputs_hook(self, layer_id, mask):\n",
        "        def fn(_, __, output):\n",
        "            modified_output = output * mask\n",
        "            self._features[layer_id] = modified_output\n",
        "            return modified_output\n",
        "        return fn\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.model(x)\n",
        "        if self.return_features:\n",
        "            return self._features\n",
        "        else:\n",
        "            return out\n",
        "        \n",
        "def generate_mask(shape, units):\n",
        "    mask = torch.ones(shape).flatten()\n",
        "    mask[units] = 0\n",
        "    return mask.reshape(shape).unsqueeze(0)  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "    FeatureExtractor class that allows you to retain outputs of any layer.\n",
        "    \n",
        "    This class uses PyTorch's \"forward hooks\", which let you insert a function\n",
        "    that takes the input and output of a module as arguements.\n",
        "    \n",
        "    In this hook function you can insert tasks like storing the intermediate values,\n",
        "    or as we'll do in the FeatureEditor class, actually modify the outputs.\n",
        "    \n",
        "    Adding these hooks can cause headaches if you don't \"remove\" them \n",
        "    after you are done with them. For this reason, the FeatureExtractor is \n",
        "    setup to be used as a context, which sets up the hooks when\n",
        "    you enter the context, and removes them when you leave:\n",
        "    \n",
        "    with FeatureExtractor(model, layer_name) as extractor:\n",
        "        features = extractor(imgs)\n",
        "    \n",
        "    If there's an error in that context (or you cancel the operation),\n",
        "    the __exit__ function of the feature extractor is executed,\n",
        "    which we've setup to remove the hooks. This will save you \n",
        "    headaches during debugging/development.\n",
        "    \n",
        "'''\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "from IPython.core.debugger import set_trace\n",
        "\n",
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self, model, layers, detach=True, clone=True, device='cpu'):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.layers = [layers] if isinstance(layers, str) else layers\n",
        "        self.detach = detach\n",
        "        self.clone = clone\n",
        "        self.device = device\n",
        "        self._features = {layer: torch.empty(0) for layer in layers}        \n",
        "        self.hooks = {}\n",
        "        \n",
        "    def hook_layers(self):        \n",
        "        self.remove_hooks()\n",
        "        for layer_id in self.layers:\n",
        "            layer = dict([*self.model.named_modules()])[layer_id]\n",
        "            self.hooks[layer_id] = layer.register_forward_hook(self.save_outputs_hook(layer_id))\n",
        "    \n",
        "    def remove_hooks(self):\n",
        "        for layer_id in self.layers:\n",
        "            self._features[layer_id] = torch.empty(0)\n",
        "            if layer_id in self.hooks:\n",
        "                self.hooks[layer_id].remove()\n",
        "                del self.hooks[layer_id]\n",
        "    \n",
        "    def __enter__(self, *args): \n",
        "        self.hook_layers()\n",
        "        return self\n",
        "    \n",
        "    def __exit__(self, *args): \n",
        "        self.remove_hooks()\n",
        "        \n",
        "    def save_outputs_hook(self, layer_id):\n",
        "        def fn(_, __, output):\n",
        "            if self.detach: output = output.detach()\n",
        "            if self.clone: output = output.clone()\n",
        "            if self.device: output = output.to(self.device)\n",
        "            self._features[layer_id] = output\n",
        "        return fn\n",
        "\n",
        "    def forward(self, x):\n",
        "        _ = self.model(x)\n",
        "        return self._features\n",
        "    \n",
        "def get_layers(model, parent_name='', layer_info=[]):\n",
        "    for module_name, module in model.named_children():\n",
        "        layer_name = parent_name + '.' + module_name\n",
        "        if len(list(module.named_children())):\n",
        "            layer_info = get_layers(module, layer_name, layer_info=layer_info)\n",
        "        else:\n",
        "            layer_info.append(layer_name.strip('.'))\n",
        "    \n",
        "    return layer_info\n",
        "\n",
        "def get_layer(m, layers):\n",
        "    layer = layers.pop(0)\n",
        "    m = getattr(m, layer)\n",
        "    if len(layers) > 0:\n",
        "        return get_layer(m, layers)\n",
        "    return m\n",
        "\n",
        "def get_layer_type(model, layer_name):\n",
        "    m = get_layer(model, layer_name.split(\".\"))\n",
        "    return m.__class__.__name__\n",
        "            \n",
        "def convert_relu_layers(parent):\n",
        "    for child_name, child in parent.named_children():\n",
        "        if isinstance(child, nn.ReLU) and child.inplace==True:\n",
        "            setattr(parent, child_name, nn.ReLU(inplace=False))\n",
        "        elif len(list(child.children())) > 0:\n",
        "            convert_relu_layers(child)"
      ],
      "metadata": {
        "id": "0Etxk1Zc6ich"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models \n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = models.alexnet(pretrained=True)\n",
        "model.to(device)\n",
        "layer_names = get_layers(model)\n",
        "layer_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SQNEbMo6p7S",
        "outputId": "82c12d25-cc9f-47a1-ac59-25a86be017bc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['features.0',\n",
              " 'features.1',\n",
              " 'features.2',\n",
              " 'features.3',\n",
              " 'features.4',\n",
              " 'features.5',\n",
              " 'features.6',\n",
              " 'features.7',\n",
              " 'features.8',\n",
              " 'features.9',\n",
              " 'features.10',\n",
              " 'features.11',\n",
              " 'features.12',\n",
              " 'avgpool',\n",
              " 'classifier.0',\n",
              " 'classifier.1',\n",
              " 'classifier.2',\n",
              " 'classifier.3',\n",
              " 'classifier.4',\n",
              " 'classifier.5',\n",
              " 'classifier.6',\n",
              " 'features.0',\n",
              " 'features.1',\n",
              " 'features.2',\n",
              " 'features.3',\n",
              " 'features.4',\n",
              " 'features.5',\n",
              " 'features.6',\n",
              " 'features.7',\n",
              " 'features.8',\n",
              " 'features.9',\n",
              " 'features.10',\n",
              " 'features.11',\n",
              " 'features.12',\n",
              " 'avgpool',\n",
              " 'classifier.0',\n",
              " 'classifier.1',\n",
              " 'classifier.2',\n",
              " 'classifier.3',\n",
              " 'classifier.4',\n",
              " 'classifier.5',\n",
              " 'classifier.6']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's get the shape of activation map for features.0\n",
        "model.eval()\n",
        "layer_name = 'features.0'\n",
        "with FeatureExtractor(model, [layer_name]) as extractor:\n",
        "  dummy_images = torch.rand(10,3,224,224)\n",
        "  output = extractor(dummy_images)\n",
        "  with torch.no_grad():\n",
        "    features = output[layer_name]\n",
        "output_map_shape = features.shape[1:]\n",
        "output_map_shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Xl-VO3a7RQQ",
        "outputId": "3e43d5b2-7d35-4d78-fd47-f90ee2a8e099"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 55, 55])"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate a mask with this same shape (with extra first dim for batch) 1x64x55x55 \n",
        "mask_units = [0,1,55]\n",
        "demo_mask = generate_mask(output_map_shape, mask_units)\n",
        "demo_mask.to(device)\n",
        "demo_mask.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4j8cDSu7tJS",
        "outputId": "16af71b9-f123-4bc7-af97-32dcb51c24aa"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 64, 55, 55])"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we're knocking out units 0,1,55 which are in the first channel\n",
        "# and should be the first two elements in the first row \n",
        "# and the first element in the second row\n",
        "demo_mask[0,0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Vcm_jAt9tG8",
        "outputId": "f899ea3d-79c9-42ee-ca95-130c786d7784"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 1.,  ..., 1., 1., 1.],\n",
              "        [0., 1., 1.,  ..., 1., 1., 1.],\n",
              "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "        ...,\n",
              "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "        [1., 1., 1.,  ..., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Here let's edit the output, and return the feature maps so we can inspect them\n",
        "model.eval()\n",
        "with FeatureEditor(model, [layer_name], [demo_mask], return_features=True) as editor:\n",
        "  # editor is now a copy of the model that will have certain activations \"zeroed out\"\n",
        "  # it does this by multilying the layer output by the mask \n",
        "  # if you have a list of layer_names, e.g., [layer1, layer2]\n",
        "  # and a list of masks, e.g., [mask1, mask2],\n",
        "  # then it's assumed you want layer1 * mask1, layer2 * mask2\n",
        "  dummy_images = torch.randn(10,3,224,224)\n",
        "  with torch.no_grad():\n",
        "    output = editor(dummy_images)\n",
        "  masked_features = output[layer_name]\n",
        "masked_features.shape  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tC0LT6qL5-RR",
        "outputId": "5b0675a1-6dc6-47c9-9ee2-52bd2aafa28b"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 64, 55, 55])"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the masked units should all have value zero\n",
        "print(masked_features[0].flatten()[mask_units])\n",
        "print(masked_features[0].flatten()[mask_units]==0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JzHzQkS9S9P",
        "outputId": "0de8e0cb-ae69-44e0-f37d-2812b82407d1"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0., 0., -0.])\n",
            "tensor([True, True, True])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now if you want to run the model, and just get the edited output \n",
        "model.eval()\n",
        "with FeatureEditor(model, [layer_name], [demo_mask], return_features=False) as editor:\n",
        "  dummy_images = torch.randn(10,3,224,224)\n",
        "  with torch.no_grad():\n",
        "    output = editor(dummy_images)\n",
        "output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0pGHeJ5Awlh",
        "outputId": "c21d8b22-9223-432a-8c58-036f25728afe"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 1000])"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# e.g., to run evaluation\n",
        "model.eval()\n",
        "with FeatureEditor(model, [layer_name], [demo_mask], return_features=False) as editor:\n",
        "  # do anything you want with the edited model...\n",
        "  # results = validate(editor, val_loader)"
      ],
      "metadata": {
        "id": "7kqtc33gBgg2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}